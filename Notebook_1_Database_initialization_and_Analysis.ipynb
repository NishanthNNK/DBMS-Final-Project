{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "DiiArlXHciEz",
      "metadata": {
        "id": "DiiArlXHciEz"
      },
      "source": [
        "# **Database initialization and ML using TF-IDF Vectorization**\n",
        "\n",
        "\n",
        "This Notebook, *Notebook 1*, efficiently processes and analyzes the Yelp dataset using Pandas for data manipulation and SQLAlchemy for database interactions. Data is read from CSV files into DataFrames, encompassing Yelp's businesses, reviews, users, tips, and check-ins. The notebook features data cleaning by removing unnecessary columns and duplicates, enhancing data integrity and storage efficiency.\n",
        "\n",
        "\n",
        "\n",
        "Key functionalities include creating a PostgreSQL database table for Illinois data to demonstrate localized analysis, and backing up the database using pg_dump for security. The notebook extends its capabilities by integrating TextBlob for sentiment analysis and applying machine learning techniques, enriching the dataset with insights into user sentiment and trends. This streamlined approach highlights the notebook's ability to handle advanced data processing and analytics within a cohesive data management framework.\n",
        "\n",
        "**Note on Dataset Limitation:**\n",
        "\n",
        "Due to the extensive size of the complete Yelp dataset, which requires substantial time and computing resources to process, our analysis has been strategically narrowed to focus solely on data from Illinois (IL). This approach allows us to manage computational demands effectively while providing a detailed example of localized data analysis.\n",
        "\n",
        "**Note on Machine Learning Model Implementation:**\n",
        "\n",
        "While there are opportunities for enhancing the machine learning model through advanced algorithms, improved feature engineering, and more sophisticated techniques, the current model serves primarily as a demonstrative tool. Its purpose is to showcase how data stored in PostgreSQL can be effectively utilized for analysis in a database management class. This implementation focuses on integrating database operations with basic machine learning to provide a practical understanding of data-driven decision."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PsoH8RyAe1Ud",
      "metadata": {
        "id": "PsoH8RyAe1Ud"
      },
      "source": [
        "**Importing necessary libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2226f7e-19a2-49e7-9127-027e14ebe3e7",
      "metadata": {
        "id": "a2226f7e-19a2-49e7-9127-027e14ebe3e7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sqlalchemy import create_engine, text\n",
        "import subprocess\n",
        "from textblob import TextBlob\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.utils import resample\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZiWsy0EAfHX8",
      "metadata": {
        "id": "ZiWsy0EAfHX8"
      },
      "source": [
        "**Pre-processing and loading of data to PostgreSQL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bc55c16-1c7b-479a-9142-5490f469d72d",
      "metadata": {
        "id": "0bc55c16-1c7b-479a-9142-5490f469d72d",
        "outputId": "fa9757d5-da14-4d3e-f490-19076da045f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Export and Backup completed\n"
          ]
        }
      ],
      "source": [
        "# Function to load JSON files\n",
        "def load_json_to_df(filepath):\n",
        "    \"\"\"\n",
        "    Load JSON data from a file into a pandas DataFrame in chunks.\n",
        "\n",
        "    Parameters:\n",
        "    - filepath: Path to the JSON file.\n",
        "\n",
        "    Returns:\n",
        "    - DataFrame containing the combined chunks.\n",
        "    \"\"\"\n",
        "    chunks = pd.read_json(filepath, lines=True, chunksize=10000)\n",
        "    df_list = []\n",
        "    for chunk in chunks:\n",
        "        df_list.append(chunk)\n",
        "    return pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "# Load JSON files directly from current directory\n",
        "df_review = load_json_to_df('review.json')\n",
        "df_user = load_json_to_df('user.json')\n",
        "df_business = load_json_to_df('business.json')\n",
        "df_tip = load_json_to_df('tip.json')\n",
        "df_checkin = load_json_to_df('checkin.json')\n",
        "\n",
        "# Function to export df to CSV\n",
        "def export_dataframe_to_csv(df, filename):\n",
        "    \"\"\"\n",
        "    Export a DataFrame to a CSV file.\n",
        "    \"\"\"\n",
        "    df.to_csv(filename, index=False)\n",
        "\n",
        "# Function to load df to PostgreSQL\n",
        "def load_dataframe_to_postgresql(df, engine, table_name, chunksize=5000):\n",
        "    \"\"\"\n",
        "    Load a pandas DataFrame into a PostgreSQL table.\n",
        "    \"\"\"\n",
        "    df.to_sql(table_name, engine, if_exists='append', index=False, chunksize=chunksize)\n",
        "\n",
        "\n",
        "# Database credentials and connection\n",
        "username = 'team1'\n",
        "password = 'yelpdata'\n",
        "host = 'localhost'\n",
        "port = '5434'\n",
        "database = 'yelpdb'\n",
        "engine = create_engine(f'postgresql://{username}:{password}@{host}:{port}/{database}')\n",
        "\n",
        "# Load CSV data into DataFrames\n",
        "df_business = pd.read_csv('business.csv', on_bad_lines='skip')\n",
        "df_review = pd.read_csv('review.csv', on_bad_lines='skip')\n",
        "df_user = pd.read_csv('user.csv', on_bad_lines='skip')\n",
        "df_tip = pd.read_csv('tip.csv', on_bad_lines='skip')\n",
        "df_checkin = pd.read_csv('checkin.csv', on_bad_lines='skip')\n",
        "\n",
        "# Data manipulation: Remove \"date\" columns and duplicates\n",
        "if 'date' in df_review.columns:\n",
        "    df_review.drop(columns=['date'], inplace=True)\n",
        "df_review.drop_duplicates(inplace=True)\n",
        "\n",
        "if 'yelping_since' in df_user.columns:\n",
        "    df_user.drop(columns=['yelping_since'], inplace=True)\n",
        "df_user.drop_duplicates(inplace=True)\n",
        "\n",
        "if 'date' in df_tip.columns:\n",
        "    df_tip.drop(columns=['date'], inplace=True)\n",
        "df_tip.drop_duplicates(inplace=True)\n",
        "\n",
        "df_business.drop_duplicates(inplace=True)\n",
        "df_checkin.drop_duplicates(inplace=True)\n",
        "\n",
        "# Load the DataFrame into PostgreSQL\n",
        "load_dataframe_to_postgresql(df_business, engine, 'business')\n",
        "load_dataframe_to_postgresql(df_review, engine, 'review')\n",
        "load_dataframe_to_postgresql(df_user, engine, 'user')\n",
        "load_dataframe_to_postgresql(df_tip, engine, 'tip')\n",
        "load_dataframe_to_postgresql(df_checkin, engine, 'checkin')\n",
        "\n",
        "# Backup the database\n",
        "subprocess.run([\n",
        "    'pg_dump',\n",
        "    '-U', username,\n",
        "    '-W',\n",
        "    '-F', 'c',\n",
        "    '-d', database,\n",
        "    '-f', 'yelpdb.dump'\n",
        "], check=True)\n",
        "\n",
        "print(\"Export and Backup completed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "L-6NcUO_fYe8",
      "metadata": {
        "id": "L-6NcUO_fYe8"
      },
      "source": [
        "**Creating a table in SQL for Illinois (IL) data, joining business_id from business and review data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6f70d1b-64f3-44a4-94a2-6441c811f09b",
      "metadata": {
        "id": "e6f70d1b-64f3-44a4-94a2-6441c811f09b",
        "outputId": "a244dae8-5da4-4ace-9c43-b96372e2dafa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Table IL_data created successfully.\n"
          ]
        }
      ],
      "source": [
        "# Database credentials\n",
        "db_username = 'postgres'\n",
        "db_password = 'yelpdata'\n",
        "db_host = 'localhost'\n",
        "db_port = '5434'\n",
        "db_name = 'yelpdb'\n",
        "\n",
        "# Create a connection to the database\n",
        "engine = create_engine(f'postgresql+psycopg2://{db_username}:{db_password}@{db_host}:{db_port}/{db_name}')\n",
        "\n",
        "# Define the SQL query\n",
        "create_table_query = text(\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS IL_data AS\n",
        "SELECT r.review_id, r.user_id, r.business_id, r.stars, r.text,\n",
        "       b.name AS business_name, b.address AS business_address, b.city AS business_city,\n",
        "       b.state AS business_state, b.categories AS business_categories,\n",
        "       u.name AS user_name\n",
        "FROM reviews r\n",
        "JOIN businesses b ON r.business_id = b.business_id\n",
        "JOIN users u ON r.user_id = u.user_id\n",
        "WHERE b.state = 'IL';\n",
        "\"\"\")\n",
        "\n",
        "# Execute the query to create a new table\n",
        "with engine.begin() as conn:  # auto-commit at the end of the block\n",
        "    conn.execute(create_table_query)\n",
        "\n",
        "print(\"Table IL_data created successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Normalization and Indexing**"
      ],
      "metadata": {
        "id": "XFuvQsIWpZRY"
      },
      "id": "XFuvQsIWpZRY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04f45dd1-97ee-4c86-8ed5-67af7a3a069b",
      "metadata": {
        "id": "04f45dd1-97ee-4c86-8ed5-67af7a3a069b",
        "outputId": "d850c4f7-89d0-4238-8d1e-1cd25bd11f51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Normalization and indexing complete.\n"
          ]
        }
      ],
      "source": [
        "# Normalization: create separate tables for businesses and users\n",
        "\n",
        "# Create businesses table\n",
        "with engine.begin() as conn:\n",
        "    conn.execute(text(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS businesses (\n",
        "        business_id VARCHAR PRIMARY KEY,\n",
        "        name VARCHAR,\n",
        "        address VARCHAR,\n",
        "        city VARCHAR,\n",
        "        state VARCHAR,\n",
        "        categories VARCHAR\n",
        "    );\n",
        "    \"\"\"))\n",
        "\n",
        "    conn.execute(text(\"\"\"\n",
        "    INSERT INTO businesses (business_id, name, address, city, state, categories)\n",
        "    SELECT DISTINCT business_id, business_name, business_address, business_city,\n",
        "           business_state, business_categories\n",
        "    FROM IL_data;\n",
        "    \"\"\"))\n",
        "\n",
        "# Create users table\n",
        "# Insert into users table, ignoring duplicates\n",
        "with engine.begin() as conn:\n",
        "    conn.execute(text(\"\"\"\n",
        "    INSERT INTO users (user_id, name)\n",
        "    SELECT DISTINCT user_id, user_name\n",
        "    FROM IL_data\n",
        "    ON CONFLICT (user_id) DO NOTHING;\n",
        "    \"\"\"))\n",
        "\n",
        "# Indexing\n",
        "\n",
        "# Create indexes for performance improvement\n",
        "with engine.begin() as conn:\n",
        "    conn.execute(text(\"CREATE INDEX IF NOT EXISTS idx_business_state ON businesses (state);\"))\n",
        "    conn.execute(text(\"CREATE INDEX IF NOT EXISTS idx_user_id ON reviews (user_id);\"))\n",
        "\n",
        "# Prepare for full-text search on the reviews table\n",
        "with engine.begin() as conn:\n",
        "    conn.execute(text(\"\"\"\n",
        "    ALTER TABLE reviews\n",
        "    ADD COLUMN IF NOT EXISTS review_text_tsvector tsvector\n",
        "    GENERATED ALWAYS AS (to_tsvector('english', text)) STORED;\n",
        "    \"\"\"))\n",
        "\n",
        "    conn.execute(text(\"CREATE INDEX IF NOT EXISTS idx_review_text ON reviews USING GIN(review_text_tsvector);\"))\n",
        "\n",
        "print(\"Normalization and indexing complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Rs3rVS-RhICs",
      "metadata": {
        "id": "Rs3rVS-RhICs"
      },
      "source": [
        "**Example Query of Business and Users Table**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bd509e5-229d-4a08-9bbf-b13c61ad0d96",
      "metadata": {
        "id": "5bd509e5-229d-4a08-9bbf-b13c61ad0d96",
        "outputId": "e0a360a4-b6bb-47c2-9a94-09546ce30763"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 5 entries in businesses table:\n",
            "              business_id                      name  \\\n",
            "0  Pns2l4eNsfO8kk83dixA6A  Abby Rappoport, LAC, CMQ   \n",
            "1  mpf3x-BjTdTEA3yCZrAYPw             The UPS Store   \n",
            "2  tUFrWirKiKi_TAnsVWINQQ                    Target   \n",
            "3  MTSW4McQd7CbVtyjqoe9mw        St Honore Pastries   \n",
            "4  mWMc6_wTdE0EUBKIGXDVfA  Perkiomen Valley Brewery   \n",
            "\n",
            "                           address           city state postal_code  \\\n",
            "0           1616 Chapala St, Ste 2  Santa Barbara    CA       93101   \n",
            "1  87 Grasso Plaza Shopping Center         Affton    MO       63123   \n",
            "2             5255 E Broadway Blvd         Tucson    AZ       85711   \n",
            "3                      935 Race St   Philadelphia    PA       19107   \n",
            "4                    101 Walnut St     Green Lane    PA       18054   \n",
            "\n",
            "    latitude   longitude  stars  review_count  is_open  \\\n",
            "0  34.426679 -119.711197    5.0             7        0   \n",
            "1  38.551126  -90.335695    3.0            15        1   \n",
            "2  32.223236 -110.880452    3.5            22        0   \n",
            "3  39.955505  -75.155564    4.0            80        1   \n",
            "4  40.338183  -75.471659    4.5            13        1   \n",
            "\n",
            "                                          categories  \n",
            "0  Doctors, Traditional Chinese Medicine, Naturop...  \n",
            "1  Shipping Centers, Local Services, Notaries, Ma...  \n",
            "2  Department Stores, Shopping, Fashion, Home & G...  \n",
            "3  Restaurants, Food, Bubble Tea, Coffee & Tea, B...  \n",
            "4                          Brewpubs, Breweries, Food  \n",
            "\n",
            "First 5 entries in users table:\n",
            "                  user_id    name  review_count  useful  funny   cool  \\\n",
            "0  qVc8ODYU5SZjKXVBgXdI7w  Walker           585    7217   1259   5994   \n",
            "1  j14WgRoU_-2ZE1aw1dXrJg  Daniel          4333   43091  13066  27281   \n",
            "2  2WnXYQFK0hXEoTxPtV2zvg   Steph           665    2086   1010   1003   \n",
            "3  SZDeASXq7o05mMNLshsdIA    Gwen           224     512    330    299   \n",
            "4  hA5lMy-EnncsH4JoR-hFGQ   Karen            79      29     15      7   \n",
            "\n",
            "                                               elite  \\\n",
            "0                                               2007   \n",
            "1  2009,2010,2011,2012,2013,2014,2015,2016,2017,2...   \n",
            "2                           2009,2010,2011,2012,2013   \n",
            "3                                     2009,2010,2011   \n",
            "4                                               None   \n",
            "\n",
            "                                             friends  fans  average_stars  \\\n",
            "0  NSCy54eWehBJyZdG2iE84w, pe42u7DcCH2QmI81NX-8qA...   267           3.91   \n",
            "1  ueRPE0CX75ePGMqOFVj6IQ, 52oH4DrRvzzl8wh5UXyU0A...  3138           3.74   \n",
            "2  LuO3Bn4f3rlhyHIaNfTlnA, j9B4XdHUhDfTKVecyWQgyA...    52           3.32   \n",
            "3  enx1vVPnfdNUdPho6PH_wg, 4wOcvMLtU6a9Lslggq74Vg...    28           4.27   \n",
            "4  PBK4q9KEEBHhFvSXCUirIw, 3FWPpM7KU1gXeOM_ZbYMbA...     1           3.54   \n",
            "\n",
            "   ...  compliment_more  compliment_profile  compliment_cute  compliment_list  \\\n",
            "0  ...               65                  55               56               18   \n",
            "1  ...              264                 184              157              251   \n",
            "2  ...               13                  10               17                3   \n",
            "3  ...                4                   1                6                2   \n",
            "4  ...                1                   0                0                0   \n",
            "\n",
            "   compliment_note  compliment_plain  compliment_cool  compliment_funny  \\\n",
            "0              232               844              467               467   \n",
            "1             1847              7054             3131              3131   \n",
            "2               66                96              119               119   \n",
            "3               12                16               26                26   \n",
            "4                1                 1                0                 0   \n",
            "\n",
            "   compliment_writer  compliment_photos  \n",
            "0                239                180  \n",
            "1               1521               1946  \n",
            "2                 35                 18  \n",
            "3                 10                  9  \n",
            "4                  0                  0  \n",
            "\n",
            "[5 rows x 21 columns]\n"
          ]
        }
      ],
      "source": [
        "# Fetch the first few rows from the businesses table\n",
        "query_businesses = \"SELECT * FROM businesses LIMIT 5;\"\n",
        "businesses_head = pd.read_sql_query(query_businesses, engine)\n",
        "print(\"First 5 entries in businesses table:\")\n",
        "print(businesses_head)\n",
        "\n",
        "# Fetch the first few rows from the users table\n",
        "query_users = \"SELECT * FROM users LIMIT 5;\"\n",
        "users_head = pd.read_sql_query(query_users, engine)\n",
        "print(\"\\nFirst 5 entries in users table:\")\n",
        "print(users_head)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hQqwkjSThWQr",
      "metadata": {
        "id": "hQqwkjSThWQr"
      },
      "source": [
        "**Query displaying created IL Table**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7265a9d-a4df-41ce-b7ef-24e4547ad560",
      "metadata": {
        "id": "c7265a9d-a4df-41ce-b7ef-24e4547ad560",
        "outputId": "5b6a7c86-af34-491a-fbd9-c2ac6df18af8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                review_id                 user_id             business_id  \\\n",
            "0  X9jSqBSMXWgHdmbnwYosvg  wqpRODS7g9lKEncUTJpJ7A  BGiQRikW27X8wfIE71iQBw   \n",
            "1  Bgk1IFHpU7t1lLAz03Ej3g  rFXFdtECXKW2mvR5qcFiow  U4ZXDA_3gIRKj6zKhelYoQ   \n",
            "2  Bgk1IFHpU7t1lLAz03Ej3g  rFXFdtECXKW2mvR5qcFiow  U4ZXDA_3gIRKj6zKhelYoQ   \n",
            "3  Bgk1IFHpU7t1lLAz03Ej3g  rFXFdtECXKW2mvR5qcFiow  U4ZXDA_3gIRKj6zKhelYoQ   \n",
            "4  5jxbkBMI9r9GT8PJ7bWC-g  vDVmry9zqbl8t4QtHyR5hA  16QLhabqmknpdVm8WwERQg   \n",
            "\n",
            "   stars                                               text  \\\n",
            "0    1.0  I recently ordered 2 pizzas, wings, and pasta ...   \n",
            "1    5.0  My family and I just moved to the area not too...   \n",
            "2    5.0  My family and I just moved to the area not too...   \n",
            "3    5.0  My family and I just moved to the area not too...   \n",
            "4    5.0  My husband and I tried this place for the firs...   \n",
            "\n",
            "          business_name    business_address business_city business_state  \\\n",
            "0             Pizza Hut      608 N Bluff Rd  Collinsville             IL   \n",
            "1  Pine Tree Laundromat  1401 S Lincoln Ave      O'Fallon             IL   \n",
            "2  Pine Tree Laundromat  1401 S Lincoln Ave      O'Fallon             IL   \n",
            "3  Pine Tree Laundromat  1401 S Lincoln Ave      O'Fallon             IL   \n",
            "4   Korean Garden Grill  2346 Mascoutah Ave    Belleville             IL   \n",
            "\n",
            "                                 business_categories user_name  \n",
            "0  Pizza, Chicken Wings, Restaurants, Fast Food, ...   Jasmine  \n",
            "1  Laundromat, Laundry Services, Local Services, ...     Kassi  \n",
            "2  Laundromat, Laundry Services, Local Services, ...     Kassi  \n",
            "3  Laundromat, Laundry Services, Local Services, ...     Kassi  \n",
            "4                                Restaurants, Korean      Jody  \n"
          ]
        }
      ],
      "source": [
        "# SQL query to fetch the first few rows\n",
        "fetch_query = \"SELECT * FROM IL_data;\"\n",
        "\n",
        "# Execute the query and load the data into a DataFrame\n",
        "IL_data = pd.read_sql_query(fetch_query, engine)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(IL_data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-45uKk_BglN3",
      "metadata": {
        "id": "-45uKk_BglN3"
      },
      "source": [
        "**Sentiment and Quality Analysis of IL Reviews**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bf7e6fd-9238-44d1-934c-3b04eb2eb348",
      "metadata": {
        "id": "8bf7e6fd-9238-44d1-934c-3b04eb2eb348",
        "outputId": "799a2398-6e57-4be8-826e-d7ab1d18a939"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                review_id                 user_id             business_id  \\\n",
            "0  X9jSqBSMXWgHdmbnwYosvg  wqpRODS7g9lKEncUTJpJ7A  BGiQRikW27X8wfIE71iQBw   \n",
            "1  Bgk1IFHpU7t1lLAz03Ej3g  rFXFdtECXKW2mvR5qcFiow  U4ZXDA_3gIRKj6zKhelYoQ   \n",
            "2  Bgk1IFHpU7t1lLAz03Ej3g  rFXFdtECXKW2mvR5qcFiow  U4ZXDA_3gIRKj6zKhelYoQ   \n",
            "3  Bgk1IFHpU7t1lLAz03Ej3g  rFXFdtECXKW2mvR5qcFiow  U4ZXDA_3gIRKj6zKhelYoQ   \n",
            "4  5jxbkBMI9r9GT8PJ7bWC-g  vDVmry9zqbl8t4QtHyR5hA  16QLhabqmknpdVm8WwERQg   \n",
            "\n",
            "   stars                                               text  \\\n",
            "0    1.0  I recently ordered 2 pizzas, wings, and pasta ...   \n",
            "1    5.0  My family and I just moved to the area not too...   \n",
            "2    5.0  My family and I just moved to the area not too...   \n",
            "3    5.0  My family and I just moved to the area not too...   \n",
            "4    5.0  My husband and I tried this place for the firs...   \n",
            "\n",
            "          business_name    business_address business_city business_state  \\\n",
            "0             Pizza Hut      608 N Bluff Rd  Collinsville             IL   \n",
            "1  Pine Tree Laundromat  1401 S Lincoln Ave      O'Fallon             IL   \n",
            "2  Pine Tree Laundromat  1401 S Lincoln Ave      O'Fallon             IL   \n",
            "3  Pine Tree Laundromat  1401 S Lincoln Ave      O'Fallon             IL   \n",
            "4   Korean Garden Grill  2346 Mascoutah Ave    Belleville             IL   \n",
            "\n",
            "                                 business_categories user_name  \\\n",
            "0  Pizza, Chicken Wings, Restaurants, Fast Food, ...   Jasmine   \n",
            "1  Laundromat, Laundry Services, Local Services, ...     Kassi   \n",
            "2  Laundromat, Laundry Services, Local Services, ...     Kassi   \n",
            "3  Laundromat, Laundry Services, Local Services, ...     Kassi   \n",
            "4                                Restaurants, Korean      Jody   \n",
            "\n",
            "   extreme_sentiment  poor_quality  likely_fake  \n",
            "0                  0             0            0  \n",
            "1                  0             1            0  \n",
            "2                  0             1            0  \n",
            "3                  0             1            0  \n",
            "4                  0             0            0  \n",
            "likely_fake\n",
            "0    154347\n",
            "1      1146\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Function to calculate sentiment extremity of reviews\n",
        "def calculate_sentiment_extremity(text, stars):\n",
        "    \"\"\"\n",
        "    Calculate sentiment polarity and subjectivity using TextBlob and categorize based on extremity.\n",
        "\n",
        "    Args:\n",
        "        text (str): The review text.\n",
        "        stars (int): The star rating of the review.\n",
        "\n",
        "    Returns:\n",
        "        int: 1 if sentiment is extreme or the text is overly simplistic for a 5-star review, 0 otherwise.\n",
        "    \"\"\"\n",
        "    analysis = TextBlob(text)\n",
        "    polarity = analysis.sentiment.polarity\n",
        "    subjectivity = analysis.sentiment.subjectivity\n",
        "\n",
        "    # Check for extreme positive/negative and subjective sentiments, or simplistic 5-star reviews\n",
        "    if ((polarity > 0.8 or polarity < -0.8) and subjectivity > 0.5) or (stars == 5 and len(text.split()) < 5):\n",
        "        return 1\n",
        "    return 0\n",
        "\n",
        "# Function to calculate review quality\n",
        "def check_review_quality(text):\n",
        "    \"\"\"\n",
        "    Check for various indicators of poor quality in the review text.\n",
        "\n",
        "    Args:\n",
        "        text (str): The review text.\n",
        "\n",
        "    Returns:\n",
        "        int: 1 if the text shows signs of poor quality, 0 otherwise.\n",
        "    \"\"\"\n",
        "    # Very short text\n",
        "    if len(text) < 50:\n",
        "        return 1\n",
        "    # High level of word repetition\n",
        "    elif len(set(text.split())) < len(text.split()) * 0.7:\n",
        "        return 1\n",
        "    # Highly subjective text might be less reliable\n",
        "    elif TextBlob(text).sentiment.subjectivity > 0.8:\n",
        "        return 1\n",
        "    # Excessive punctuation used\n",
        "    elif bool(re.search(r'[\\!\\?]{2,}', text)):\n",
        "        return 1\n",
        "    return 0\n",
        "\n",
        "# Apply refined sentiment analysis considering stars\n",
        "IL_data['extreme_sentiment'] = IL_data.apply(\n",
        "    lambda x: calculate_sentiment_extremity(x['text'], x['stars']), axis=1\n",
        ")\n",
        "\n",
        "# Apply improved quality checks\n",
        "IL_data['poor_quality'] = IL_data['text'].apply(check_review_quality)\n",
        "\n",
        "# Adjust the criteria for likely fake reviews\n",
        "IL_data['likely_fake'] = IL_data[['extreme_sentiment', 'poor_quality']].sum(axis=1)\n",
        "IL_data['likely_fake'] = IL_data['likely_fake'].apply(lambda x: 1 if x >= 2 else 0)\n",
        "\n",
        "# Display the outcome\n",
        "print(IL_data.head())\n",
        "print(IL_data['likely_fake'].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "T31PYYsdhg68",
      "metadata": {
        "id": "T31PYYsdhg68"
      },
      "source": [
        "**Preprocessing and Data Splitting**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb06992b-0c1c-4a54-a1f2-4f52fd85e41c",
      "metadata": {
        "id": "bb06992b-0c1c-4a54-a1f2-4f52fd85e41c"
      },
      "outputs": [],
      "source": [
        "# 'text' column to be vectorized and 'stars', 'extreme_sentiment', and 'poor_quality' are other features\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # Limiting features to avoid overfitting\n",
        "\n",
        "# Create a transformer for combining TF-IDF vectorization and scaling other numeric features\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('txt', tfidf_vectorizer, 'text'),\n",
        "        ('num', StandardScaler(), ['stars', 'extreme_sentiment', 'poor_quality'])\n",
        "    ])\n",
        "\n",
        "# Prepare features\n",
        "X = preprocessor.fit_transform(IL_data)\n",
        "y = IL_data['likely_fake']  # Target variable\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9CuqGgf4hlQF",
      "metadata": {
        "id": "9CuqGgf4hlQF"
      },
      "source": [
        "**Handling Imbalanced Data and Model Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97fd6458-3350-4924-bba5-f90234d7a180",
      "metadata": {
        "id": "97fd6458-3350-4924-bba5-f90234d7a180",
        "outputId": "3542c58a-3389-4aab-eb77-a953e26ab96d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     30867\n",
            "           1       1.00      0.99      0.99       232\n",
            "\n",
            "    accuracy                           1.00     31099\n",
            "   macro avg       1.00      0.99      1.00     31099\n",
            "weighted avg       1.00      1.00      1.00     31099\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Apply SMOTE to the training data to address class imbalance\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Train the RandomForest classifier with class weight adjustment for further balancing\n",
        "model = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
        "model.fit(X_train_smote, y_train_smote)\n",
        "\n",
        "# Predict on the test set and print the classification report\n",
        "y_pred = model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XN1xvTGghzMO",
      "metadata": {
        "id": "XN1xvTGghzMO"
      },
      "source": [
        "**Class Balancing by Downsampling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5748d2a-fe20-45a6-9ee0-736b9c01cc92",
      "metadata": {
        "id": "e5748d2a-fe20-45a6-9ee0-736b9c01cc92",
        "outputId": "d9a024ae-a715-4b4e-f424-8559d4cd5dcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "likely_fake\n",
            "0    1146\n",
            "1    1146\n",
            "Name: count, dtype: int64\n",
            "New class counts after downsampling:\n",
            "likely_fake\n",
            "0    1146\n",
            "1    1146\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Separate majority and minority classes\n",
        "df_majority = IL_data[IL_data.likely_fake == 0]\n",
        "df_minority = IL_data[IL_data.likely_fake == 1]\n",
        "\n",
        "# Downsample majority class\n",
        "df_majority_downsampled = resample(df_majority,\n",
        "                                   replace=False,    # sample without replacement\n",
        "                                   n_samples=len(df_minority),  # to match minority class\n",
        "                                   random_state=42)  # reproducible results\n",
        "\n",
        "# Combine minority class with downsampled majority class\n",
        "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
        "\n",
        "# Display new class counts\n",
        "print(df_downsampled.likely_fake.value_counts())\n",
        "\n",
        "# Use this balanced data for model training\n",
        "X = preprocessor.fit_transform(df_downsampled)\n",
        "y = df_downsampled['likely_fake']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "\n",
        "# Separate majority and minority classes\n",
        "df_majority = IL_data[IL_data['likely_fake'] == 0]\n",
        "df_minority = IL_data[IL_data['likely_fake'] == 1]\n",
        "\n",
        "# Downsample majority class without replacement to match minority class size\n",
        "df_majority_downsampled = resample(df_majority,\n",
        "                                   replace=False,  # Sample without replacement\n",
        "                                   n_samples=len(df_minority),  # Match minority class size\n",
        "                                   random_state=42)  # Ensure reproducibility\n",
        "\n",
        "# Combine the downsampled majority class with the minority class\n",
        "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
        "\n",
        "# Display new class counts to verify balancing\n",
        "print(\"New class counts after downsampling:\")\n",
        "print(df_downsampled['likely_fake'].value_counts())\n",
        "\n",
        "# Use the balanced data for model training\n",
        "X = preprocessor.fit_transform(df_downsampled)\n",
        "y = df_downsampled['likely_fake']\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "K53fViXUh_O2",
      "metadata": {
        "id": "K53fViXUh_O2"
      },
      "source": [
        "**Cross-Validation using StatifiedKFold**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ab3c18b-7b7f-4e25-aa0d-4e5b16f04b88",
      "metadata": {
        "id": "9ab3c18b-7b7f-4e25-aa0d-4e5b16f04b88",
        "outputId": "51d015e1-6e60-4bce-e25f-814936b41dc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average F1-score: 0.989526784703837\n"
          ]
        }
      ],
      "source": [
        "# Initialize the cross-validator with 5 splits, maintaining the same class proportions\n",
        "cv = StratifiedKFold(n_splits=5)\n",
        "\n",
        "# Compute the F1-score using macro averaging across folds\n",
        "scores = cross_val_score(model, X, y, cv=cv, scoring='f1_macro')\n",
        "\n",
        "# Display the average F1-score across all folds\n",
        "print(\"Average F1-score:\", scores.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IiFTnxreiD6f",
      "metadata": {
        "id": "IiFTnxreiD6f"
      },
      "source": [
        "**Cross-Validation using StatifiedShuffleSplit**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99312b00-32e4-4ad8-bf5d-9ddc132be9c0",
      "metadata": {
        "id": "99312b00-32e4-4ad8-bf5d-9ddc132be9c0",
        "outputId": "3d83997c-f070-476e-b63e-34e7a2100bc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Revised average F1-score: 0.9995642618795596\n"
          ]
        }
      ],
      "source": [
        "# Initialize the stratified shuffle splitter for cross-validation\n",
        "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the RandomForest classifier with balanced class weights\n",
        "model = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
        "\n",
        "# Compute the F1-score using macro averaging, over random stratified splits\n",
        "scores = cross_val_score(model, X, y, cv=sss, scoring='f1_macro')\n",
        "\n",
        "# Display the revised average F1-score across splits\n",
        "print(\"Revised average F1-score:\", scores.mean())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}